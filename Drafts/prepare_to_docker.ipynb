{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.relative_locator import locate_with\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\AppData\\Local\\Temp\\ipykernel_22312\\868309719.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path =r\"chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path =r\"chromedriver.exe\")\n",
    "initial_url = \"https://religion.ranepa.ru/home/archive/2017/418813/\"\n",
    "driver.get(initial_url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "turn_english = [i for i in driver.find_elements(By.CLASS_NAME, 'carte__language') if i.text == 'En'][0]\n",
    "turn_english.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigSoup = BeautifulSoup(driver.page_source, 'lxml') # Получаем html код из Selenium и адаптируем под BS4. Чтобы если что в Google Colab было проще залить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old function \n",
    "# def get_article_info(article_html_part) -> dict:\n",
    "#     \"\"\" Функция принимает на вход кусочек html одной статьи.\n",
    "#         Вытаскивает из этого html кусочка данные.\n",
    "#         Формирует словарь.\n",
    "#         Потом это пригодиться чтобы сформировать xml файл.\"\"\"\n",
    "#     try:\n",
    "#         year = re.search(r'20\\d\\d', initial_url)[0]\n",
    "#     except:\n",
    "#         year = 'what is year'\n",
    "\n",
    "\n",
    "#     soup = BeautifulSoup(article_html_part.get_attribute('outerHTML'), 'lxml')\n",
    "\n",
    "\n",
    "#     # Articles Title\n",
    "#     try:\n",
    "#         article_title = soup.find('div', 'name').text.strip()\n",
    "#         # Добавить здесь преоброзование строки unicode (Add here trnasformer / encoder from uncode to assii)\n",
    "#         # Но xml не ломается из-за этого и unicode отоброжает корректно. Поэтому пока не вставил\n",
    "#     except:\n",
    "#         article_title = ''\n",
    "\n",
    "#     # Authors names\n",
    "#     try:\n",
    "#         # article_author_name = article.find_element(By.CLASS_NAME, 'author').text\n",
    "#         article_author_name = soup.find('div', 'author').text.strip()\n",
    "#     except:\n",
    "#         article_author_name = ''\n",
    "\n",
    "#     # Dict of articles PDF's Links. Ru + En if it exists\n",
    "#     try:\n",
    "#         # article_pdf_ru_link = article.find_elements(By.CLASS_NAME, 'lang')\n",
    "#         # article_pdf_links = { i.text : i.get_attribute('href') for j in article_pdf_ru_link for i in j.find_elements(By.CSS_SELECTOR, 'a')}\n",
    "#         article_pdf_links = {link.text.strip(): 'https://religion.ranepa.ru' + link['href'] for link in soup.find('div', 'lang').find_all('a', href=True)}\n",
    "\n",
    "#     except:\n",
    "#         print('Кажется не нашел ссылку на какую-то статью')\n",
    "#         article_pdf_links = ''\n",
    "\n",
    "\n",
    "#     # Abstract\n",
    "#     try:\n",
    "#         # absttrect_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Abstract\"))\n",
    "#         abstract_text = ''.join([i for i in soup.find('div', 'archive__row').find('b', text=\"Abstract\").find_next('br').find_next('br').next_sibling]).strip()\n",
    "#         if abstract_text[-1] != '.': # Тэги на некоторые старницых криво стоят, и чтобы аннотация парсилась целиком нужно\n",
    "#             abstract_text = abstract_text + ''.join([i for i in soup.find('div', 'archive__row').find('b', text=\"Abstract\").find_next('br').find_next('br').find_next('br').next_sibling]).strip()\n",
    "\n",
    "#         if abstract_text == '-':\n",
    "#             abstract_text = 'N/A'\n",
    "#     except:\n",
    "#         abstract_text = 'N/A'\n",
    "\n",
    "#     # DOI: ****-*****-*****-****\n",
    "#     try:\n",
    "#         doi_parse = soup.find('div', 'archive__row').find('b', text=re.compile(\"DOI:\"))\n",
    "#         doi_parse = doi_parse.text.strip()\n",
    "#     except:\n",
    "#         doi_parse = ''\n",
    "\n",
    "#     # Key Words\n",
    "#     try:\n",
    "#         # keyword_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Keywords\"))\n",
    "#         KeyWordss = soup.find('div', 'archive__row').find('b', text=re.compile(\"Keywords\")).find_next('i')\n",
    "#         KeyWordss = KeyWordss.text.strip()\n",
    "#     except:\n",
    "#         KeyWordss = ''\n",
    "\n",
    "#     # Section: Main or somthing else\n",
    "#     try:\n",
    "#         # section_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Section:\"))\n",
    "#         section_text = soup.find('div', 'archive__row').find('b', text=re.compile(\"Section:\")).find_next('i')\n",
    "#         section_text = section_text.text.strip()\n",
    "#     except:\n",
    "#         section_text = ''\n",
    "\n",
    "#     # Topic\n",
    "#     try:\n",
    "#         # topic_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Section:\"))\n",
    "#         topic_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Topic:\")).find_next('i')\n",
    "#         topic_title = topic_title.text.strip()\n",
    "#     except:\n",
    "#         topic_title = ''\n",
    "\n",
    "#     # Pages:\n",
    "#     try:\n",
    "#         # pages_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Pages:\"))\n",
    "#         pages_text = soup.find('div', 'archive__row').find('b', text=re.compile(\"Pages:\")).find_next('i')\n",
    "#         pages_text = pages_text.text.strip()\n",
    "#     except:\n",
    "#         pages_text = ''\n",
    "\n",
    "#     article\n",
    "\n",
    "\n",
    "#     global current_iter\n",
    "#     current_iter += 1\n",
    "\n",
    "#     # Создаем словарь в котором будут хранитьсч вот эти данные.\n",
    "#     article_data_dict = {\n",
    "#                         'number': current_iter, # Номер статьи которая идет по счету на web странице и в общем зачете. от 1 до 687\n",
    "#                         'Year': year,\n",
    "#                         'Article Title': article_title,\n",
    "#                         'Author':article_author_name,\n",
    "#                         'KeyWords': KeyWordss,\n",
    "#                         'DOI': doi_parse[4:].strip(), # Так как BS4 находит тэг целиком с словом и цифрами типо такого - DOI: 987134-890-348950, то букаы нам не нужны. Обрубаем их\n",
    "#                         'Section': section_text,\n",
    "#                         'Topic': topic_title,\n",
    "#                         'Pages': pages_text,\n",
    "#                         'Abstract': abstract_text,\n",
    "#                         'Links': article_pdf_links,\n",
    "#                     }\n",
    "#     return article_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(article_html_part) -> dict:\n",
    "    \"\"\" Функция принимает на вход кусочек html одной статьи.\n",
    "        Вытаскивает из этого html кусочка данные.\n",
    "        Формирует словарь. \n",
    "        Потом это пригодиться чтобы сформировать xml файл.\"\"\"\n",
    "\n",
    "    # Issue №\n",
    "    try:\n",
    "        issue_number = bigSoup.find('a', 'archive__subhead-link active').text\n",
    "    except:\n",
    "        issue_number = 'issue number has not found'\n",
    "\n",
    "    # Year \n",
    "    try:\n",
    "        year = re.search(r'20\\d\\d', initial_url)[0]\n",
    "    except:\n",
    "        year = 'what is year'\n",
    "        \n",
    "\n",
    "    \n",
    "    # soup = BeautifulSoup(article_html_part.get_attribute('outerHTML'), 'lxml') \n",
    "    soup = article_html_part\n",
    "    \n",
    "    \n",
    "    # Articles Title\n",
    "    try:\n",
    "        article_title = soup.find('div', 'name').text.strip() \n",
    "        # Добавить здесь преоброзование строки unicode (Add here trnasformer / encoder from uncode to assii)\n",
    "        # Но xml не ломается из-за этого и unicode отоброжает корректно. Поэтому пока не вставил\n",
    "    except:\n",
    "        article_title = ''\n",
    "\n",
    "    # Authors names\n",
    "    try:\n",
    "        # article_author_name = article.find_element(By.CLASS_NAME, 'author').text\n",
    "        article_author_name = soup.find('div', 'author').text.strip()\n",
    "    except:\n",
    "        article_author_name = ''\n",
    "    \n",
    "    # Key Words\n",
    "    try:\n",
    "        # keyword_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Keywords\")) \n",
    "        KeyWordss = soup.find('b', text=re.compile(\"Keywords\")).find_next('i')\n",
    "        KeyWordss = KeyWordss.text.strip()\n",
    "    except:\n",
    "        KeyWordss = ''\n",
    "\n",
    "    # DOI: ****-*****-*****-****\n",
    "    try:\n",
    "        doi_parse = soup.find('b', text=re.compile(\"DOI:\")) \n",
    "        doi_parse = doi_parse.text.strip()\n",
    "    except:\n",
    "        doi_parse = ''\n",
    "\n",
    "        \n",
    "    # Section: Main or somthing else\n",
    "    try:\n",
    "        # section_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Section:\")) \n",
    "        section_text = soup.find('b', text=re.compile(\"Section:\")).find_next('i')\n",
    "        section_text = section_text.text.strip()\n",
    "    except:\n",
    "        section_text = ''\n",
    "\n",
    "    # Topic \n",
    "    try:\n",
    "        # topic_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Section:\")) \n",
    "        topic_title = soup.find('b', text=re.compile(\"Topic:\")).find_next('i')\n",
    "        topic_title = topic_title.text.strip()\n",
    "    except:\n",
    "        topic_title = ''\n",
    "\n",
    "    # Pages:\n",
    "    try:\n",
    "        # pages_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Pages:\"))\n",
    "        pages_text = soup.find('b', text=re.compile(\"Pages:\")).next_sibling\n",
    "        pages_text = pages_text.text.strip()\n",
    "    except:\n",
    "        pages_text = ''\n",
    "\n",
    "    # Abstract\n",
    "    try:\n",
    "        # absttrect_title = soup.find('div', 'archive__row').find('b', text=re.compile(\"Abstract\")) \n",
    "        abstract_text = ''.join([i for i in soup.find('b', text=\"Abstract\").find_next('br').find_next('br').next_sibling]).strip()\n",
    "        if abstract_text[-1] != '.': # Тэги на некоторые старницых криво стоят, и чтобы аннотация парсилась целиком нужно \n",
    "            abstract_text = abstract_text + ''.join([i for i in soup.find('b', text=\"Abstract\").find_next('br').find_next('br').find_next('br').next_sibling]).strip()\n",
    "        \n",
    "        if abstract_text == '-':\n",
    "            abstract_text = 'N/A'\n",
    "    except:\n",
    "        abstract_text = 'N/A'\n",
    "\n",
    "\n",
    "    # lINKS pdf. Dict of articles PDF's Links. Ru + En if it exists\n",
    "    try:\n",
    "        # article_pdf_ru_link = article.find_elements(By.CLASS_NAME, 'lang')\n",
    "        # article_pdf_links = { i.text : i.get_attribute('href') for j in article_pdf_ru_link for i in j.find_elements(By.CSS_SELECTOR, 'a')}\n",
    "        article_pdf_links = {link.text.strip(): 'https://religion.ranepa.ru' + link['href'] for link in soup.find('div', 'lang').find_all('a', href=True)}\n",
    "\n",
    "    except:\n",
    "        print('Кажется не нашел ссылку на какую-то статью')\n",
    "        article_pdf_links = ''\n",
    "     \n",
    "        \n",
    "    \n",
    "    global current_iter\n",
    "    current_iter += 1\n",
    "\n",
    "    # Создаем словарь в котором будут хранитьсч вот эти данные.\n",
    "    article_data_dicts = {\n",
    "                        'number': current_iter, # Номер статьи которая идет по счету на web странице и в общем зачете. от 1 до 687\n",
    "                        'Issue number': issue_number,\n",
    "                        'Year': year,\n",
    "                        'Article Title': article_title, \n",
    "                        'Author':article_author_name, \n",
    "                        'KeyWords': KeyWordss,\n",
    "                        'DOI': doi_parse[4:].strip(), # Так как BS4 находит тэг целиком с словом и цифрами типо такого - DOI: 987134-890-348950, то букаы нам не нужны. Обрубаем их\n",
    "                        'Section': section_text,\n",
    "                        'Topic': topic_title,\n",
    "                        'Pages': pages_text,\n",
    "                        'Abstract': abstract_text,\n",
    "                        'Links': article_pdf_links, \n",
    "                    }\n",
    "    article_data_dict = copy.deepcopy(article_data_dicts)\n",
    "    return article_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем функцию которая генерирует тот второй словарь с именами, фамилиями и email афторов.\n",
    "def from_article_info_to_dict_authors(article_info_dict_1: dict) -> list:\n",
    "    \"\"\" Функция формирует список словарей с инофрмацией об авторах на основе article_info (ранее собранных данных). \n",
    "        Информация об авторах - это (имя, фамилия, email).\n",
    "        Пока что email у всех пустой.\n",
    "        \n",
    "        Функция возвращает список словарей с информацией об авторах\"\"\"\n",
    "\n",
    "\n",
    "    dict_authors = []\n",
    "    for key, value in article_info_dict_1.items():\n",
    "        if key == 'Author':\n",
    "            if value == ' ' or value == '' or len(value.split(', ')) == 0:\n",
    "                print('None')\n",
    "                name_data = {\n",
    "                    'name': '',\n",
    "                    'surname': '',\n",
    "                    'email': '',\n",
    "                }\n",
    "                dict_authors.append(name_data)\n",
    "            elif len(value.split(', ')) >= 2:\n",
    "                # print('Bigger 2')\n",
    "                name_data = []\n",
    "                name_1 = value.split(', ')\n",
    "                name_2 = {}\n",
    "                number_of_author = 1\n",
    "                for name_surname in name_1:\n",
    "                    # print(name_surname)\n",
    "                    name_2['name'] = ' '.join(\n",
    "                        name_surname.strip().split(' ')[0:-1])\n",
    "                    name_2['surname'] = name_surname.strip().split(' ')[-1]\n",
    "                    name_2['email'] = ''\n",
    "                    name_3 = copy.deepcopy(name_2)\n",
    "                    # name_data[str(number_of_author)] = name_2\n",
    "                    number_of_author += 1\n",
    "                    dict_authors.append(name_3)\n",
    "                    # print(name_2)\n",
    "            elif len(value.split(', ')) == 1:\n",
    "                # print('One')\n",
    "                name_data = {}\n",
    "                name_2 = {}\n",
    "                name_2['name'] = ' '.join(value.strip().split(' ')[0:-1])\n",
    "                name_2['surname'] = value.strip().split(' ')[-1]\n",
    "                name_2['email'] = ''\n",
    "                dict_authors.append(name_2)\n",
    "            elif len(value.split(', ')) == 0:\n",
    "                # print('None')\n",
    "                name_data = {}\n",
    "                name_2 = {}\n",
    "                name_2['name'] = ''\n",
    "                name_2['surname'] = value[:]\n",
    "                name_2['email'] = ''\n",
    "                dict_authors.append(name_2)\n",
    "    dict_authors = copy.deepcopy(dict_authors)\n",
    "    return dict_authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.dirname('xml/')):\n",
    "    os.makedirs('xml')\n",
    "\n",
    "def generate_taplate_for_xml_file(article_info_dict_1: dict) -> None:\n",
    "\n",
    "    dict_authors = copy.deepcopy(from_article_info_to_dict_authors(article_info_dict_1))\n",
    "    # Функция from_article_info_to_dict_authors() писали до этого. Возврашает список словарей с инфой по авторам.\n",
    "\n",
    "    root = ET.Element('article', xmlns_xlink=\"http://www.w3.org/1999/xlink\", dtd_version=\"1.1\" ) # [1] [1]\n",
    "    front = ET.SubElement(root, 'front')\n",
    "\n",
    "    journal_meta = ET.SubElement(front, 'journal-meta')\n",
    "    journal_id = ET.SubElement(journal_meta, 'journal-id', journal_id_type=\"publisher\") # [1]\n",
    "    journal_id.text = 'GRC' # Здесь пишем название издательства\n",
    "\n",
    "    journal_title_group = ET.SubElement(journal_meta, 'journal-title-group')\n",
    "    journal_title = ET.SubElement(journal_title_group, 'journal-title')\n",
    "    journal_title.text = \"Gosudarstvo, religiia, tserkov' v Rossii i za rubezhom \" # Здесь пишем название Журнала.\n",
    "\n",
    "    abbrev_journal_title = ET.SubElement(journal_title_group, 'abbrev-journal-title', abbrev_type='nlm-ta') # [1] | Что-то особенное нужно вставить в abbrev-type?\n",
    "\n",
    "    isnn_ppub = ET.SubElement(journal_meta, 'issn', pub_type=\"ppub\")\n",
    "    isnn_epub = ET.SubElement(journal_meta, 'issn', pub_type=\"epub\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    article_meta = ET.SubElement(front, 'article-meta')\n",
    "    article_id = ET.SubElement(article_meta, 'article-id', pub_id_type='manuscript') # [1]\n",
    "    article_id.text = '' # Какой код сюда вставить?\n",
    "\n",
    "    article_categories = ET.SubElement(article_meta, 'article-categories')\n",
    "    subj_group_article_type = ET.SubElement(article_categories, 'subj-group', subj_group_type=\"article_type\") # [1]\n",
    "    subject = ET.SubElement(subj_group_article_type, 'subject')\n",
    "    subject.text = 'Research Letter' # Что сюда вставиьт ? \n",
    "\n",
    "\n",
    "    subj_group_subject_areas = ET.SubElement(article_categories, 'subj-group', subj_group_type=\"subject_areas\") # [1]\n",
    "    subject_areas = ' ' #['area1', 'area2'] # для теста. сюда вставить данные по subject areas если будет\n",
    "    for word in subject_areas:\n",
    "        subject = ET.SubElement(subj_group_subject_areas, 'subject')\n",
    "        subject.text = word # Вставить всюда subject_areas \n",
    "\n",
    "\n",
    "    title_group = ET.SubElement(article_meta, 'title-group')\n",
    "    article_title = ET.SubElement(title_group, 'article-title')\n",
    "    article_title.text = article_info_dict_1['Article Title']   #'Название статьи' \n",
    "\n",
    "    alt_title = ET.SubElement(title_group, 'alt-title', alt_title_type=\"running\") # [1]\n",
    "    alt_title.text = '' # Вставить какое-то alt название ???\n",
    "\n",
    "\n",
    "\n",
    "    contrib_group = ET.SubElement(article_meta, 'contrib-group')\n",
    "    # dict_authors = [\n",
    "    # \t{'name': \"Aleksey\",\n",
    "    # \t'surname': 'Kalinov',\n",
    "    # \t'email': '',\n",
    "    # \t}, \n",
    "    # \t{'name': \"Petr\",\n",
    "    # \t'surname': 'Vorlamov',\n",
    "    # \t'email': '',\n",
    "    # \t}\n",
    "    # ]\n",
    "    for author in dict_authors:\n",
    "        # print(author['name'], \n",
    "        #     author['surname'], \n",
    "        #     dict_authors.index(author)+1)\n",
    "        contrib = ET.SubElement(contrib_group, 'contrib', contrib_type=\"author\" ) # [1]\n",
    "        name_xml = ET.SubElement(contrib, 'name')\n",
    "        \n",
    "        try:\n",
    "            surname_xml = ET.SubElement(name_xml, 'surname')\n",
    "            surname_xml.text = str(author['surname'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            given_names = ET.SubElement(name_xml, 'given-names')\n",
    "            given_names.text = str(author['name'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            email_xml = ET.SubElement(contrib, 'email')\n",
    "            email_xml.text = str(author['email'])\n",
    "        except:\n",
    "            pass\n",
    "        role_contrib = ET.SubElement(contrib, 'role', content_type=str(dict_authors.index(author)+1)) # [1]\n",
    "        aff_number = 'aff'+ str(dict_authors.index(author)+1)\n",
    "        xref = ET.SubElement(contrib, 'xref', ref_type='aff', rid=aff_number)\n",
    "\n",
    "\n",
    "    for author in dict_authors:\n",
    "        aff_number = 'aff'+ str(dict_authors.index(author)+1)\n",
    "        aff = ET.SubElement(contrib_group, 'aff', id=aff_number)\n",
    "        institution = ET.SubElement(aff, 'institution')\n",
    "        if 'institution' in author:\n",
    "            institution.text = author['institution'] # Где взять институт ? Указать Ранхигс или нужен универ атвора? \n",
    "\n",
    "        addrline1 = ET.SubElement(aff, 'addr-line', content_type=\"addrline1\") # [1]\n",
    "        if 'addrline1' in author:\n",
    "            addrline1.text = author['addrline1'] # Где взять адресс ? Указать адресс универа? \n",
    "\n",
    "        city_xml = ET.SubElement(aff, 'addr-line', content_type='city')\n",
    "        if 'city' in author:\n",
    "            city_xml.text = author['city']\n",
    "\n",
    "        state_xml = ET.SubElement(aff, 'addr-line', content_type='state')\n",
    "        if 'state' in author:\n",
    "            state_xml.text = author['state']\n",
    "\n",
    "        zipcode_xml = ET.SubElement(aff, 'addr-line', content_type='zipcode')\n",
    "        if 'zipcode' in author:\n",
    "            zipcode_xml.text = author['zipcode']\n",
    "\n",
    "        country_xml = ET.SubElement(aff, 'addr-line', content_type='country')\n",
    "        if 'country' in author:\n",
    "            city_xml.text = author['country']\n",
    "\n",
    "\n",
    "    if 'coresp' in author:\n",
    "        autor_notes = ET.SubElement(article_meta, 'author-notes')\n",
    "        coresp_id = ET.SubElement(autor_notes, id='cor1')\n",
    "        label_coresp = ET.SubElement(coresp_id, 'label')\n",
    "        label_coresp.text = '*'\n",
    "        bold_coresp = ET.SubElement(coresp_id, 'bold')\n",
    "        bold_coresp.text = 'Corresponding Author'\n",
    "        after_bold = ET.SubElement(coresp_id)\n",
    "        after_bold.text = ', '.replace(map(str, [value for key, value in author.items()]))\n",
    "\n",
    "\n",
    "    pub_date_epub = ET.SubElement(article_meta, 'pub-date', pub_type='epub') # [1]\n",
    "    pub_date_ppub = ET.SubElement(article_meta, 'pub-date', pub_type='ppub') # [1]\n",
    "\n",
    "\n",
    "    elocation_id = ET.SubElement(article_meta, 'elocation-id')\n",
    "    elocation_id.text = article_id.text # Примудать от куда взять это ID он такой же как и в article_id.text\n",
    "\n",
    "\n",
    "    history = ET.SubElement(article_meta, 'history')\n",
    "    date_xml = ET.SubElement(history, 'date', data_type='received')\n",
    "    day_xml = ET.SubElement(date_xml, 'day')\n",
    "    day_xml.text = '' # Где брать дату? \n",
    "    month_xml = ET.SubElement(date_xml, 'month')\n",
    "    month_xml.text = '' # Где брать дату?\n",
    "    year_xml = ET.SubElement(date_xml, 'year')\n",
    "    # year_xml.text = '' # Где брать дату? | Год взял из парсенного журанала\n",
    "    year_xml.text = article_info_dict_1['Year']\n",
    "\n",
    "\n",
    "    permission_xml = ET.SubElement(article_meta, 'permissions')\n",
    "    copyright_statement = ET.SubElement(permission_xml, 'copyright-statement')\n",
    "    copyright_statement.text = '' # Где брать копирайт выражение? Оно есть у ГРЦ? \n",
    "    copyright_year = ET.SubElement(permission_xml, 'copyright-year')\n",
    "    copyright_year.text = '' # Где брать год для коипарайта? Или указывать 2022, но не уверен, что это законно \n",
    "\n",
    "    #Absract \n",
    "    abstract_xml = ET.SubElement(article_meta, 'abstract')\n",
    "    abstract_b_xml = ET.SubElement(abstract_xml, 'b')\n",
    "    if len(article_info_dict_1['Abstract']) > 1:\n",
    "        abstract_b_xml.text = article_info_dict_1['Abstract']\n",
    "    else:\n",
    "        abstract_b_xml.text = 'N/A' \n",
    "\n",
    "    key_words_group = ET.SubElement(article_meta, 'kwd-group')\n",
    "\n",
    "\n",
    "    # article_test = {\n",
    "    # \t'title': 'The numerous artifacts in the sky of peoples mind',\n",
    "    # \t'key_words': ['word1', 'drugs', 'math', 'comupterScience']}\n",
    "    try:\n",
    "        for key_word in article_info_dict_1['KeyWords'].split(', '):\n",
    "            kwd = ET.SubElement(key_words_group, 'kwd')\n",
    "            kwd.text = key_word\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    counts_xml = ET.SubElement(article_meta, 'counts')\n",
    "    table_count = ET.SubElement(counts_xml, 'table-count' , count='0') # Считать таблицы или забить ?\n",
    "    page_count = ET.SubElement(counts_xml, 'page-count', count='0') # Вроде нужно считать старницы, но в примере Джэксона стоят 0. Может что-то другое считают, а навазвание совпало. Ну или они подзабивают на точность и аккуратность данных в xml документах\n",
    "\n",
    "    def prettify(element, indent='  '):\n",
    "        \"\"\" Фцнкция преобразовывает одностроничный xml в красивый многострочный xml с отсупыми\"\"\"\n",
    "        queue = [(0, element)]  # (level, element)\n",
    "        while queue:\n",
    "            level, element = queue.pop(0)\n",
    "            children = [(level + 1, child) for child in list(element)]\n",
    "            if children:\n",
    "                element.text = '\\n' + indent * (level+1)  # for child open\n",
    "            if queue:\n",
    "                element.tail = '\\n' + indent * queue[0][0]  # for sibling open\n",
    "            else:\n",
    "                element.tail = '\\n' + indent * (level-1)  # for parent close\n",
    "            queue[0:0] = children  # prepend so children come before siblings\n",
    "\n",
    "\n",
    "    prettify(root)\n",
    "\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write('xml/sample.xml', encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanning_and_save_xml_file(article_info_dict_2: dict,\n",
    "                                num_of_article: int | str,\n",
    "                                num_of_issue: int | str) -> None:\n",
    "    \"\"\" Функция очищает файл Sample.xml от ломающих xml кодов.\n",
    "\n",
    "        Функция дорабаватывает xml файлов, там где '_' ставит '-' (Сразу не получилось поставить из-за особенностей Python)\n",
    "\n",
    "        Функция сохраняет чистые и красивые  xml файлы под порядковым номером статьи. (номер формируется автоматически) \n",
    "\n",
    "        Функция формирует папку с ошибьками и файл с описанием ошибки. \n",
    "        Если во время сохранения возникли ошибки, то их тоже сохранет в папку erorrs. \n",
    "        Если ошибок не было, то и папки erorrs нет\"\"\"\n",
    "        \n",
    "    number_of_current_articles_counter = article_info_dict_2['number']\n",
    "    year_xmll = article_info_dict_2['Year']\n",
    "    # num_of_issue = article_info_dict_2['Issue number']\n",
    "    # Read in the file\n",
    "    with open('xml/sample.xml', 'r', encoding='UTF-8') as file :\n",
    "        filedata = file.read()\n",
    "\n",
    "    for_replace_in_xml = {\n",
    "    \"xmlns_xlink\": 'xmlns:xlink',\n",
    "    \"dtd_version\": \"dtd-version\",\n",
    "    \"ournal_id_type\": 'journal-id-type',\n",
    "    'abbrev_type' : 'abbrev-type',\n",
    "    'pub_id_type' : 'pub-id-type',\n",
    "    'subj_group_type': 'subj-group-type',\n",
    "    'subj_group_type':'subj-group-type',\n",
    "    'alt_title_type':'alt-title-type', \n",
    "    'contrib_type':'contrib-type',\n",
    "    'content_type':'content-type', # Не 100% будет\n",
    "    'pub_type':'pub-type',\n",
    "    'pub_type':'pub-type',\n",
    "    'data_type':'data-type',\n",
    "    'ref_type': 'ref-type',\n",
    "    '\u0002': '\"', # из-за этой истории крашиться xml документ\n",
    "    }\n",
    "\n",
    "    for wrong, right in for_replace_in_xml.items():\n",
    "        filedata = filedata.replace(wrong, right)\n",
    "    # # Заменяем косячные слова на правильные с дефисом\n",
    "    # filedata = filedata.replace('dtd_version', 'dtd-version')\n",
    "\n",
    "    # Write the file out again\n",
    "    # path_for_saving_lxml = f'lxml/lxml_{number_of_current_articles_counter}.xml'\n",
    "\n",
    "    filename_1 = f'xml/{year_xmll}/{num_of_issue}/'\n",
    "    \n",
    "    if len(article_info_dict_2['Author']) <= 0 and len(article_info_dict_2['Abstract']) <=3:\n",
    "        \n",
    "        filename_erroes = f'xml/{year_xmll}/{num_of_issue}/errors/'\n",
    "        path_for_saving_lxml = f'xml/{year_xmll}/{num_of_issue}/errors/{num_of_article}_lxml_{number_of_current_articles_counter}.xml'\n",
    "\n",
    "        # Формируем txt файл где пытаемся сказать что не так и дать какую-то отладочную информацию\n",
    "        dict_that_contains_data_from_article_info = {\n",
    "                \"article_info_dict_2['Article Title']\": len(article_info_dict_2['Article Title']), \n",
    "                \"article_info_dict_2['Author']\": len(article_info_dict_2['Author']),\n",
    "                \"article_info_dict_2['Links']\": len(article_info_dict_2['Links']), \n",
    "                \"article_info_dict_2['KeyWords']\": len(article_info_dict_2['KeyWords']), \n",
    "                \"article_info_dict_2['DOI']\": len(article_info_dict_2['DOI']), \n",
    "                \"article_info_dict_2['Section']\": len(article_info_dict_2['Section']), \n",
    "                \"article_info_dict_2['Topic']\": len(article_info_dict_2['Topic']), \n",
    "                \"article_info_dict_2['Pages']\": len(article_info_dict_2['Pages']), \n",
    "                \"article_info_dict_2['Abstract']\": len(article_info_dict_2['Abstract']), \n",
    "                \"article_info_dict_2['Year']\": len(article_info_dict_2['Year']), \n",
    "        }\n",
    "        filedata_erorrs = []\n",
    "        filedata_erorrs_2 = []\n",
    "        # Формируем файл для отладки. Чтобы было проще искать где файл\n",
    "        filedata_erorrs.append(f'Что-то пошло не так.{path_for_saving_lxml} \\nГод: {year_xmll} \\nВыпуск №: {num_of_issue}\\nСтатья в выпуске №: {num_of_article}\\nСтатья в общем подсчете№: {number_of_current_articles_counter} \\n\\n\\n')\n",
    "        \n",
    "        # Формируем данные что именно выглядит подозриетльно\n",
    "        for article_info_key, len_article_info_value in dict_that_contains_data_from_article_info.items():\n",
    "            if len_article_info_value <= 1:\n",
    "                filedata_erorrs.append(f'Не найдено (или подозрительно короткое):{article_info_key[19:]}\\n')\n",
    "        \n",
    "            dict_that_contains_data_from_article_info_2 = {\n",
    "                    \"article_info_dict_2['Article Title']\": article_info_dict_2['Article Title'], \n",
    "                    \"article_info_dict_2['Author']\": article_info_dict_2['Author'],\n",
    "                    \"article_info_dict_2['Links']\": article_info_dict_2['Links'], \n",
    "                    \"article_info_dict_2['KeyWords']\": article_info_dict_2['KeyWords'], \n",
    "                    \"article_info_dict_2['DOI']\": article_info_dict_2['DOI'], \n",
    "                    \"article_info_dict_2['Section']\": article_info_dict_2['Section'], \n",
    "                    \"article_info_dict_2['Topic']\": article_info_dict_2['Topic'], \n",
    "                    \"article_info_dict_2['Pages']\": article_info_dict_2['Pages'], \n",
    "                    \"article_info_dict_2['Abstract']\": article_info_dict_2['Abstract'], \n",
    "                    \"article_info_dict_2['Year']\": article_info_dict_2['Year'], \n",
    "            }\n",
    "\n",
    "            for article_info_key_2, len_article_info_value_2 in dict_that_contains_data_from_article_info_2 .items():\n",
    "                if len_article_info_value <= 1:\n",
    "                    filedata_erorrs_2.append(f'{article_info_key_2[19:]}: '+ str(len_article_info_value_2) + '\\n')\n",
    "\n",
    "\n",
    "        filedata_erorrs.append('\\n\\nЧто храниться в этой дате\\n\\n')\n",
    "        filedata_erorrs.append(' '.join(set(filedata_erorrs_2)))\n",
    "\n",
    "        filedata_erorrs = ' '.join(filedata_erorrs)\n",
    "        \n",
    "        path_for_saving_txt = f'xml/{year_xmll}/{num_of_issue}/errors/{num_of_article}_lxml_{number_of_current_articles_counter}.txt'\n",
    "        if not os.path.exists(os.path.dirname(filename_erroes)):\n",
    "            os.makedirs(filename_erroes)\n",
    "        \n",
    "\n",
    "        with open(path_for_saving_lxml, 'w', encoding='UTF-8') as file:\n",
    "            file.write(filedata)\n",
    "        \n",
    "        with open(path_for_saving_txt, 'w', encoding='UTF-8') as file:\n",
    "            file.write(filedata_erorrs)\n",
    "    else:\n",
    "        if not os.path.exists(os.path.dirname(filename_1)):\n",
    "            os.makedirs(filename_1)\n",
    "        path_for_saving_lxml = f'xml/{year_xmll}/{num_of_issue}/{num_of_article}_lxml_{number_of_current_articles_counter}.xml'\n",
    "\n",
    "        with open(path_for_saving_lxml, 'w', encoding='UTF-8') as file:\n",
    "            file.write(filedata)\n",
    "    return 'hello saving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(os.path.dirname('xml')):\n",
    "#     os.makedirs('xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://religion.ranepa.ru/'\n",
    "\n",
    "# Находит ссылки на архив каждого года\n",
    "archive_all_years = {year_link.text: base_url + year_link['href'] for year_link in bigSoup.find_all('a', 'sidebar__year-number', href=True)}\n",
    "\n",
    "#  Находим ссылки на выпуски\n",
    "issues_links = {issue.text: base_url + issue['href'] for issue in bigSoup.find_all('a', 'archive__subhead-link') if issue.text != 'Download all'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'number': 1, 'Issue number': '#3 (35)', 'Year': '2017', 'Article Title': 'Editorial', 'Author': '', 'KeyWords': '', 'DOI': '', 'Section': '', 'Topic': '', 'Pages': '-', 'Abstract': 'N/A', 'Links': {'Ru': 'https://religion.ranepa.ru/upload/iblock/36b/1_1.pdf'}}, {'number': 2, 'Issue number': '#3 (35)', 'Year': '2017', 'Article Title': 'Islamic Reformation: The Heuristic Value of the Approach', 'Author': 'Irina Starodubrovskaya', 'KeyWords': 'Reformation, Islamic fundamentalism, protestant ethic, modernization, individualism, generation conflict.', 'DOI': '', 'Section': 'Main theme', 'Topic': 'Contemporary islam: paradoxes of reformation', 'Pages': '-', 'Abstract': 'This article explores the analytical value of Islamic reformation as a conceptual framework for the analysis of the current situation in the Islamic world. Different attitudes toward this concept are compared, resulting in the conclusion that any valid assessment requires a definition of reformation as a religious and social process that can exist outside the specific period of the Protestant Reformation. Delineation of the general characteristics of reformation makes it possible to demonstrate that current tendencies in the Islamic world are similar to those occurring during the Reformation and that Islamic fundamentalists are the driving force of this new reformation. This means that it is incorrect to assess their values and their influence as archaic, reactionary, or fascist. The worldview of Islamic fundamentalists contains both modernizing and anti-modernizing features, and some elements of their influence on society are of a modernizing nature.', 'Links': {'Ru': 'https://religion.ranepa.ru/upload/iblock/095/GRC_3-2017_Ready+.2_0.pdf'}}, {'number': 3, 'Issue number': '#3 (35)', 'Year': '2017', 'Article Title': '“Islamic Reformation”: Positive Project or Artificial Concept?', 'Author': 'Sofya Ragozina', 'KeyWords': 'Islam, Islamic studies, Islamic Reformation, Islamic modernism, orientalism, ijtihad.', 'DOI': '', 'Section': 'Main theme', 'Topic': 'Contemporary islam: paradoxes of reformation', 'Pages': '-', 'Abstract': 'Today the concept of “Islamic Reformation” acts as a universal framework for a large number of research projects within the field of Islamic and Muslim studies. This theory, mediated by Western modernization theory, claims a comprehensive understanding of Islamic reality and thus attracts many researchers. However, this universality results in a lack of attention to some important areas, which stimulates criticism from experts on Islam. The aim of this article is to identify and characterize the main approaches to understanding the phenomenon of “Islamic reformation.” There are three different groups of researchers who accept the concept’s validity, among whom there is no unity regarding its content. The first group talks about “Islamic reformation” as a positive political program, while the second connects it exclusively with the negative phenomenon of Islamic radicalism. The third group does not engage in polemics about “Islamic reformation,” but rather consistently seeks to prove the concept’s inadequacy in explaining Islamic realities and to offer alternative research models. A detailed consideration of a number of works demonstrates several different approaches within one discourse on “Islamic reformation”.', 'Links': {'Ru': 'https://religion.ranepa.ru/upload/iblock/054/GRC_3-2017_Ready+.3.pdf'}}, {'number': 4, 'Issue number': '#3 (35)', 'Year': '2017', 'Article Title': 'Islam in the West or Western Islam? The Disconnect of Religion and Culture', 'Author': 'Olivier Roy', 'KeyWords': 'Islam, Islam in Europe, multiculturalism, fundamentalism.', 'DOI': '', 'Section': 'Main theme', 'Topic': 'Contemporary islam: paradoxes of reformation', 'Pages': '-', 'Abstract': 'The definitive presence of a huge Muslim population in Europe will have long-term consequences. There is, nevertheless, some debate about the size of the Muslim population, partly due to imprecise data, partly due to the difficulty of knowing who qualifies as Muslim. Is one defined as a Muslim because of one’s choice of religious community, or is one a Muslim by ethnic background? Beyond the demographic aspect, the fact that Islam is taking hold in Europe seems to call into question European identity. What does the rise of Islam in Europe entail in terms of shared culture and values? Should we speak of “Islam in the West” as if Islam were the bridgehead for a different culture, or of “Western Islam” as if a European Islam would necessarily differ from its Middle Eastern or Asian versions?', 'Links': {'Ru': 'https://religion.ranepa.ru/upload/iblock/049/4_Roy_2017_3_GRC.pdf'}}, {'number': 5, 'Issue number': '#3 (35)', 'Year': '2017', 'Article Title': 'The Post-Secular Age of the Neomodern in the Middle East', 'Author': 'Vasiliy Kuznetzov', 'KeyWords': 'secularism, post-secularism, postmodern, modern, archaic, neomodern, Islam, Middle East, Arab Awakening.', 'DOI': '', 'Section': 'Main theme', 'Topic': 'Contemporary islam: paradoxes of reformation', 'Pages': '-', 'Abstract': 'This article is dedicated to an analysis of the current situation in the Arab world within the framework of Neomodernism theory and to the detection of the religious component of the socio-political process. According to Neomodernism theory, contemporary human society is at the point of transferring from the postmodern stage to a new one. This stage is characterized by the combination of three elements: the need for a new positive message, the archaic content of this message, and the use of postmodern tools to construct it. The religious factor plays a special and very important role in the socio-political process, which can be discovered by means of secular and post-secular ideas. After making some general observations the author tries to apply them to Middle Eastern reality and Arab-Muslim culture. As a result he outlines the principal traits of “Islamic secularism” and reveals the common post-secular trends of Middle Eastern and Western society. At the end of the article the author analyzes links between Neomodernism and post-secularism at the regional and global levels.', 'Links': {'Ru': 'https://religion.ranepa.ru/upload/iblock/659/4_Kuznetsov_2017_3_GRC.pdf'}}]\n"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "page_data = [] # Данные статей заугруженной страницы\n",
    "for article in bigSoup.find_all('div', 'archive__row'):\n",
    "    page_data.append(get_article_info(article))\n",
    "    # if current_iter > 2:\n",
    "    #     break\n",
    "\n",
    "# print(list(all_data[-1].values())[:1])\n",
    "print(page_data[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tag.find of <a class=\"archive__subhead-link active\" href=\"/home/archive/2012/418074/\">#1 (30)</a>> <class 'method'>\n"
     ]
    }
   ],
   "source": [
    "article_info = get_article_info(article)\n",
    "article_info['Author'] = '  kAPITAN hIMOV, Abdil Afanasevich bludov, koravanov'\n",
    "from_article_info_to_dict_authors(article_info)\n",
    "x = bigSoup.select('a.archive__subhead-link.active')\n",
    "x = bigSoup.find(True, {'class': 'archive__subhead-link'}).find(True, {'class': 'dict_authors'})\n",
    "# issue_number = bigSoup.find('a', 'archive__subhead-link active').text\n",
    "print(x, type(x))\n",
    "# article_info = get_article_info(article)\n",
    "# generate_taplate_for_xml_file(article_info)\n",
    "# cleanning_and_save_xml_file(article_info,\n",
    "#                             num_of_article=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://religion.ranepa.ru//home/archive/2012/418074/\n",
      "https://religion.ranepa.ru//home/archive/2012/418101/\n",
      "https://religion.ranepa.ru//home/archive/2012/418151/\n"
     ]
    }
   ],
   "source": [
    "issues_links = {issue.text: base_url + issue['href'] for issue in bigSoup.find_all(\n",
    "    'a', 'archive__subhead-link')if issue.text != 'Download all'}\n",
    "for link in issues_links.values():\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 4 (840420763.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [271]\u001b[1;36m\u001b[0m\n\u001b[1;33m    for num in numberss:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 4\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "numberss = [1, 2, 4, 5]\n",
    "for i in tqdm(range(len(numberss)), desc=\"Text You Want\"):\n",
    "\n",
    "for num in numberss:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "current_iter = 0\n",
    "\n",
    "for year_text, year_link in archive_all_years.items():\n",
    "    sleep(1.5)\n",
    "    initial_url = year_link\n",
    "    driver.get(initial_url)\n",
    "\n",
    "    bigSoup = BeautifulSoup(driver.page_source, 'lxml') \n",
    "    issues_links = {issue.text: base_url + issue['href'] for issue in bigSoup.find_all('a', 'archive__subhead-link')if issue.text != 'Download all'}\n",
    "\n",
    "    for text, link in issues_links.items():\n",
    "\n",
    "        sleep(1)\n",
    "        initial_url = link\n",
    "        driver.get(initial_url)\n",
    "\n",
    "        bigSoup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        num_of_article = 1\n",
    "\n",
    "        for article in bigSoup.find_all('div', 'archive__row'):\n",
    "            article_info = copy.deepcopy(get_article_info(article))\n",
    "            generate_taplate_for_xml_file(article_info)\n",
    "            cleanning_and_save_xml_file(article_info, num_of_article = num_of_article, num_of_issue=text)\n",
    "            num_of_article += 1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Matt\\\\Desktop\\\\selnoid_grc'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('Введите что-то: '  )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотел сделать программу, что будет давать выбор что делать, но сразу не заработало, а разбьираться уже нет времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_years():\n",
    "    global current_iter\n",
    "    current_iter = 0\n",
    "\n",
    "    for year_text, year_link in archive_all_years.items():\n",
    "        sleep(1.5)\n",
    "        initial_url = year_link\n",
    "        driver.get(initial_url)\n",
    "\n",
    "        bigSoup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        issues_links = {issue.text: base_url + issue['href'] for issue in bigSoup.find_all(\n",
    "            'a', 'archive__subhead-link')if issue.text != 'Download all'}\n",
    "\n",
    "        for text, link in issues_links.items():\n",
    "\n",
    "            sleep(1)\n",
    "            initial_url = link\n",
    "            driver.get(initial_url)\n",
    "\n",
    "            bigSoup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            num_of_article = 1\n",
    "\n",
    "            for article in bigSoup.find_all('div', 'archive__row'):\n",
    "                article_info = copy.deepcopy(get_article_info(article))\n",
    "                generate_taplate_for_xml_file(article_info)\n",
    "                cleanning_and_save_xml_file(\n",
    "                    article_info, num_of_article=num_of_article)\n",
    "                num_of_article += 1\n",
    "\n",
    "\n",
    "def parse_one_page(url_page: str):\n",
    "    global current_iter\n",
    "    current_iter = 0\n",
    "    num_of_article = 1\n",
    "\n",
    "    driver.get(url_page)\n",
    "    sleep(7)\n",
    "\n",
    "    for article in bigSoup.find_all('div', 'archive__row'):\n",
    "        article_info = copy.deepcopy(get_article_info(article))\n",
    "        generate_taplate_for_xml_file(article_info)\n",
    "        cleanning_and_save_xml_file(\n",
    "            article_info, num_of_article=num_of_article)\n",
    "        num_of_article += 1\n",
    "\n",
    "\n",
    "print(f\"Привет\\n Эта программа помогает собирать и преоброзовывать данные с сайта журнала: Госудраство Религия Церьковь\")\n",
    "print(f'У этой программы есть две способности')\n",
    "print('1. Первая возможность это собрать и преоброзовать все статьи за все года и все выпуски \\n По времени займет примерно 3-5 минут. Возможно быстрее, если мощный комп ')\n",
    "print('2. Вторая возможность это собрать и преоброзвать статьи с той ссылки которую скините. Пример был на прекрепленом видосе. \\nПо времени займет секунд 20')\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Итак чтобы выбрать 1 способ, введите 1 \\n А чтобы второй, то просто вставьте ссылку\")\n",
    "action_input = input('Вводить здесь: ').strip()\n",
    "\n",
    "if 'https://religion.ranepa.ru/' in action_input:\n",
    "    print('Выбран способ 2. \\nНачинается сбор и преобразования данных в xml')\n",
    "    print('Во время работы прогнраммы, здесь могут появлятся слова None. \\nЕсли такое случитсья не переживайте, все хорошго, так и должно быть')\n",
    "else:\n",
    "    print('Вырае способ 1. \\nНачинается сбор и преобразования данных в xml')\n",
    "    print('Во время работы прогнраммы, здесь могут появлятся слова None. \\nЕсли такое случитсья не переживайте, все хорошго, так и должно быть')\n",
    "print(f'Все спарсилось, смотритве в папке:\\n {os.getcwd()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('grc-YWM_p044-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cf100dc0670a21d08a9eaecdf36474ef0c1a9df3324d3b46f5f24ebd0296165"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
